

<!DOCTYPE html>


<html lang="ja" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />
<meta property="og:title" content="画像生成 AI 入門: Python による拡散モデルの理論と実践" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://shunk031.me/coloso-python-diffusion-models/lectures/section-04-12.html" />
<meta property="og:site_name" content="画像生成AI入門: Pythonによる拡散モデルの理論と実践" />
<meta property="og:description" content="Open In Colab Section 04. Key Researches Based on Non-Diffusion Models:&quot;非&quot;拡散モデルベースの手法や、拡散モデルを支える基盤モデルについて紹介します。 Lecture 12. About CLIP (Contrastive Language-Image Pre-training): CLIP モデルのダウンロードと実行、..." />
<meta property="og:image" content="https://raw.githubusercontent.com/shunk031/coloso-python-diffusion-models/main/assets/logo.png" />
<meta property="og:image:alt" content="画像生成AI入門: Pythonによる拡散モデルの理論と実践" />
<meta name="description" content="Open In Colab Section 04. Key Researches Based on Non-Diffusion Models:&quot;非&quot;拡散モデルベースの手法や、拡散モデルを支える基盤モデルについて紹介します。 Lecture 12. About CLIP (Contrastive Language-Image Pre-training): CLIP モデルのダウンロードと実行、..." />

    <title>画像生成 AI 入門: Python による拡散モデルの理論と実践 &#8212; 画像生成AI入門: Pythonによる拡散モデルの理論と実践</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-8ZY8TGCYMR"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-8ZY8TGCYMR');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/section-04-12';</script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="検索" href="../search.html" />
    <link rel="next" title="画像生成 AI 入門: Python による拡散モデルの理論と実践" href="section-06-18.html" />
    <link rel="prev" title="画像生成 AI 入門: Python による拡散モデルの理論と実践" href="section-03-11.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ja"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../README.html">
                    画像生成 AI 入門：Python による拡散モデルの理論と実践
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Section 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="section-01-03.html">Play with Stable Diffusion!</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Section 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="section-02-05.html">About Deep Learning (2)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Section 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="section-03-08.html">Score-based Generative Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="section-03-09.html">Denoising Diffusion Probabilistic Model (1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="section-03-10.html">Denoising Diffusion Probabilistic Model (2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="section-03-11.html">Beyond Conventional GANs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Section 4</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">About CLIP</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Section 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="section-06-18.html">Components of Stable Diffusion</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Section 7</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="section-07-19.html">Stable Diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="section-07-20.html">Textual Inversion</a></li>
<li class="toctree-l1"><a class="reference internal" href="section-07-21.html">DreamBooth</a></li>
<li class="toctree-l1"><a class="reference internal" href="section-07-22.html">Attend-and-Excite</a></li>
<li class="toctree-l1"><a class="reference internal" href="section-07-23.html">ControlNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="section-07-24.html">Prompt-to-Prompt</a></li>
<li class="toctree-l1"><a class="reference internal" href="section-07-25.html">InstructPix2Pix</a></li>
<li class="toctree-l1"><a class="reference internal" href="section-07-26.html">unCLIP</a></li>
<li class="toctree-l1"><a class="reference internal" href="section-07-27.html">Paint-by-example</a></li>
<li class="toctree-l1"><a class="reference internal" href="section-07-28.html">LoRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="section-07-29.html">Safe Latent Diffusion</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">受講者特典</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../materials/figures/README.html">高解像度図表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../materials/notebooks/activation-functions.html">活性化関数のグラフの形状</a></li>
<li class="toctree-l1"><a class="reference internal" href="../materials/notebooks/linear-regression.html">線形回帰</a></li>
<li class="toctree-l1"><a class="reference internal" href="../materials/notebooks/normal-distribution.html">多次元正規分布の描画</a></li>
<li class="toctree-l1"><a class="reference internal" href="../materials/notebooks/noise-scheduler.html">ノイズスケジューラの動作確認</a></li>
<li class="toctree-l1"><a class="reference internal" href="../materials/notebooks/diffusion-process.html">拡散過程の可視化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../materials/notebooks/distribution-shift.html">分布シフト</a></li>
<li class="toctree-l1"><a class="reference internal" href="../materials/notebooks/latent-data.html">潜在データ (latent data) の可視化</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/shunk031/coloso-python-diffusion-models/blob/main/lectures/section-04-12.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/shunk031/coloso-python-diffusion-models" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="ソースリポジトリ"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/shunk031/coloso-python-diffusion-models/issues/new?title=Issue%20on%20page%20%2Flectures/section-04-12.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="問題を報告"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="このページをダウンロード">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/section-04-12.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="ソースファイルをダウンロード"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="PDFに印刷"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="全画面モード"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="検索" aria-label="検索" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>画像生成 AI 入門: Python による拡散モデルの理論と実践</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> 目次 </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-04-key-researches-based-on-non-diffusion-models">Section 04. Key Researches Based on Non-Diffusion Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lecture-12-about-clip-contrastive-language-image-pre-training">Lecture 12. About CLIP (Contrastive Language-Image Pre-training)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">セットアップ</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu">GPU が使用できるか確認</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python">利用する Python ライブラリをインストール</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clip">CLIP モデルの読み込み</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clipprocessor">CLIPProcessor について</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clipimageprocessor">CLIPImageProcessor による画像処理</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cliptokenizer">CLIPTokenizer による言語処理</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">画像とテキストのセットアップ</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">特徴量の構築</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">コサイン類似度を計算する</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot">Zero-shot 画像分類</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="ai-python">
<h1>画像生成 AI 入門: Python による拡散モデルの理論と実践<a class="headerlink" href="#ai-python" title="Permalink to this heading">#</a></h1>
<p><a class="reference external" href="https://colab.research.google.com/github/shunk031/coloso-python-diffusion-models/blob/main/lectures/section-04-12.ipynb" rel="noreferrer" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<section id="section-04-key-researches-based-on-non-diffusion-models">
<h2>Section 04. Key Researches Based on Non-Diffusion Models<a class="headerlink" href="#section-04-key-researches-based-on-non-diffusion-models" title="Permalink to this heading">#</a></h2>
<p>&quot;非&quot;拡散モデルベースの手法や、拡散モデルを支える基盤モデルについて紹介します。</p>
<section id="lecture-12-about-clip-contrastive-language-image-pre-training">
<h3>Lecture 12. About CLIP (Contrastive Language-Image Pre-training)<a class="headerlink" href="#lecture-12-about-clip-contrastive-language-image-pre-training" title="Permalink to this heading">#</a></h3>
<p>CLIP モデルのダウンロードと実行、任意の画像とテキスト入力の類似度計算、zero-shot 画像分類の実行方法を紹介します。</p>
</section>
</section>
<section id="id1">
<h2>セットアップ<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<section id="gpu">
<h3>GPU が使用できるか確認<a class="headerlink" href="#gpu" title="Permalink to this heading">#</a></h3>
<p>本 Colab ノートブックを実行するために GPU ランタイムを使用していることを確認します。CPU ランタイムと比べて画像生成がより早くなります。以下の <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> コマンドが失敗する場合は再度講義資料の <code class="docutils literal notranslate"><span class="pre">GPU</span> <span class="pre">使用設定</span></code> のスライド説明や Google Colab の <a class="reference external" href="https://research.google.com/colaboratory/faq.html#gpu-utilization" rel="noreferrer" target="_blank">FAQ</a> 等を参考にランタイムタイプが正しく変更されているか確認してください。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>!nvidia-smi
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Wed Jun 21 12:18:05 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   45C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</pre></div>
</div>
</div>
</div>
</section>
<section id="python">
<h3>利用する Python ライブラリをインストール<a class="headerlink" href="#python" title="Permalink to this heading">#</a></h3>
<p>今回は diffusers は使いません。その代わりに diffusers でも使われていて、diffusers をメンテナンスしている huggingface 社の transformers を使用します。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>!pip install transformers
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting transformers
  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">7.2/7.2 MB</span> <span class=" -Color -Color-Red">97.7 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>
?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)
Collecting huggingface-hub&lt;1.0,&gt;=0.14.1 (from transformers)
  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">236.8/236.8 kB</span> <span class=" -Color -Color-Red">32.6 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>
?25hRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)
Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)
Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)
Collecting tokenizers!=0.11.3,&lt;0.14,&gt;=0.11.1 (from transformers)
  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">7.8/7.8 MB</span> <span class=" -Color -Color-Red">123.9 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>
?25hCollecting safetensors&gt;=0.3.1 (from transformers)
  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">1.3/1.3 MB</span> <span class=" -Color -Color-Red">88.5 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>
?25hRequirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)
Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.14.1-&gt;transformers) (2023.4.0)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.14.1-&gt;transformers) (4.5.0)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (1.26.15)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2022.12.7)
Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2.0.12)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.4)
Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers
Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="clip">
<h2>CLIP モデルの読み込み<a class="headerlink" href="#clip" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://huggingface.co/docs/transformers/index" rel="noreferrer" target="_blank">🤗 Huggingface transformers</a> を使用して、 CLIP モデルによる zero-shot 画像分類（学習データには含まれていないような、完全に未知のデータを対象）に取り組みます。</p>
<p>以下、<a class="reference external" href="https://github.com/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb" rel="noreferrer" target="_blank">Interacting with CLIP</a> を参考に動作を追っていきます。ここではまず初めに <code class="docutils literal notranslate"><span class="pre">CLIPModel</span></code> で <code class="docutils literal notranslate"><span class="pre">openai/clip-vit-large-patch14</span></code> を読み込みます。その後、読み込んだモデルに対応するテキストと画像の処理を行う <code class="docutils literal notranslate"><span class="pre">CLIPProcessor</span></code> を読み込みます。</p>
<p>今回使用する事前学習済み CLIP モデル以外にも様々なものが <a class="reference external" href="https://huggingface.co/models?other=clip" rel="noreferrer" target="_blank">huggingface hub</a> 上で見つけることができます。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">CLIPModel</span><span class="p">,</span> <span class="n">CLIPProcessor</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;openai/clip-vit-large-patch14&quot;</span>

<span class="c1"># CLIP モデルの読み込み</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CLIPModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="c1"># モデルを推論モードにする</span>
<span class="c1"># このとき dropout を無効化したり、batch normalization の動作を推論用にする</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># CLIP 用の前処理 pipeline の読み込み</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">CLIPProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "386ff688e554429092f2846667372b59"}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[&quot;id2label&quot;]` will be overriden.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "2f4f39e786654577a0536170f447e2b0"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "024a06fdb47b4c0f9141f64833dc3e66"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "8a9538feeecd4d9e96cf1cc02e443057"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "5887db8c37a64549af740fa7f40cfcd6"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "cafb435cf72d46ec9a9568aa4619458f"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "1615de49a27a42309dab78b1dc660c8d"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "d9d724c2d5854f4a8e757f100b7db435"}</script></div>
</div>
<p>読み込んだモデルのパラメータ数、入力画像の解像度、入力テキストの最大長、語彙数を確認します。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model parameters: </span><span class="si">{</span><span class="nb">sum</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()])</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input resolution: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vision_config</span><span class="o">.</span><span class="n">image_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Context length: </span><span class="si">{</span><span class="n">processor</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocab size: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">text_config</span><span class="o">.</span><span class="n">vocab_size</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model parameters: 427,616,513
Input resolution: 224
Context length: 77
Vocab size: 49,408
</pre></div>
</div>
</div>
</div>
</section>
<section id="clipprocessor">
<h2>CLIPProcessor について<a class="headerlink" href="#clipprocessor" title="Permalink to this heading">#</a></h2>
<p>CLIP は画像とテキストを扱うマルチモーダルモデルです。それぞれのモダリティを適切に入力できるように、<a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPProcessor" rel="noreferrer" target="_blank"><code class="docutils literal notranslate"><span class="pre">CLIPProcessor</span></code></a> を使用して統一的にマルチモーダルデータを扱います。</p>
<p>以下は <code class="docutils literal notranslate"><span class="pre">CLIPProcessor</span></code> の概要です。画像の前処理を司る <code class="docutils literal notranslate"><span class="pre">CLIPImageProcessor</span></code> とテキストの前処理を司る <code class="docutils literal notranslate"><span class="pre">CLIPTokenizerFast</span></code> が含まれています。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">processor</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CLIPProcessor:
- image_processor: CLIPImageProcessor {
  &quot;crop_size&quot;: {
    &quot;height&quot;: 224,
    &quot;width&quot;: 224
  },
  &quot;do_center_crop&quot;: true,
  &quot;do_convert_rgb&quot;: true,
  &quot;do_normalize&quot;: true,
  &quot;do_rescale&quot;: true,
  &quot;do_resize&quot;: true,
  &quot;feature_extractor_type&quot;: &quot;CLIPFeatureExtractor&quot;,
  &quot;image_mean&quot;: [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  &quot;image_processor_type&quot;: &quot;CLIPImageProcessor&quot;,
  &quot;image_std&quot;: [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  &quot;resample&quot;: 3,
  &quot;rescale_factor&quot;: 0.00392156862745098,
  &quot;size&quot;: {
    &quot;shortest_edge&quot;: 224
  }
}

- tokenizer: CLIPTokenizerFast(name_or_path=&#39;openai/clip-vit-large-patch14&#39;, vocab_size=49408, model_max_length=77, is_fast=True, padding_side=&#39;right&#39;, truncation_side=&#39;right&#39;, special_tokens={&#39;bos_token&#39;: AddedToken(&quot;&lt;|startoftext|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=True), &#39;eos_token&#39;: AddedToken(&quot;&lt;|endoftext|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=True), &#39;unk_token&#39;: AddedToken(&quot;&lt;|endoftext|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=True), &#39;pad_token&#39;: &#39;&lt;|endoftext|&gt;&#39;}, clean_up_tokenization_spaces=True)
</pre></div>
</div>
</div>
</div>
<section id="clipimageprocessor">
<h3>CLIPImageProcessor による画像処理<a class="headerlink" href="#clipimageprocessor" title="Permalink to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">CLIPProcessor</span></code> に含まれる画像処理機構 <code class="docutils literal notranslate"><span class="pre">CLIPImageProcessor</span></code> の動作を確認します。まずはサンプルとなる画像をダウンロードします。ここでは stable diffusion 本家レポジトリから画像を借りました。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/txt2img/000002025.png&quot;</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">raw</span><span class="p">)</span>
<span class="n">image</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/427e10286097e0334a7721d9d61aed74a7386fcca97a3d49d8a41fbe93ecbb72.png" src="../_images/427e10286097e0334a7721d9d61aed74a7386fcca97a3d49d8a41fbe93ecbb72.png" />
</div>
</div>
<p>ダウンロードしたサンプル画像は 1024 x 512 の解像度を有しています。CLIP の入力解像度は 224 x 224 であるため、この画像はリサイズされるべきです。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">image</span><span class="o">.</span><span class="n">size</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1024, 512)
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">CLIPImageProcessor</span></code> の機能を使って CLIP の入力に合うように前処理します。ここでは <code class="docutils literal notranslate"><span class="pre">CLIPProcessor</span></code> の <code class="docutils literal notranslate"><span class="pre">images</span></code> 引数に対象の画像を渡すことで、内部的に <code class="docutils literal notranslate"><span class="pre">CLIPImageProcessor</span></code> を用いて処理を行います。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">output</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;pixel_values&#39;: tensor([[[[ 1.7260,  0.3391, -0.3908,  ...,  0.9376,  1.0106,  1.1566],
          [ 1.7406,  0.7917, -0.3616,  ...,  0.9230,  0.9668,  1.0690],
          [ 1.7114,  1.4194, -0.0550,  ...,  0.8938,  0.9084,  1.0252],
          ...,
          [-1.4565, -1.3689, -1.5003,  ..., -1.6609, -1.6317, -1.5441],
          [-1.5149, -1.4419, -1.4711,  ..., -1.6463, -1.6025, -1.4565],
          [-1.4857, -1.4711, -1.4273,  ..., -1.6463, -1.6025, -1.6025]],

         [[ 2.0449,  1.1294,  0.4540,  ...,  1.6847,  1.7297,  1.8348],
          [ 2.0299,  1.5046,  0.4841,  ...,  1.6847,  1.7147,  1.8047],
          [ 2.0149,  1.9398,  0.8442,  ...,  1.6697,  1.6997,  1.8047],
          ...,
          [-1.3169, -1.1818, -1.3469,  ..., -1.6170, -1.5570, -1.4820],
          [-1.3169, -1.2118, -1.3019,  ..., -1.6170, -1.5420, -1.3019],
          [-1.2568, -1.1818, -1.2568,  ..., -1.5870, -1.4970, -1.4219]],

         [[ 2.0037,  1.2785,  0.7523,  ...,  1.6909,  1.7051,  1.8046],
          [ 2.0321,  1.5771,  0.7523,  ...,  1.6909,  1.6909,  1.7904],
          [ 2.0037,  1.9184,  1.0510,  ...,  1.6766,  1.6909,  1.7762],
          ...,
          [-1.0678, -0.9256, -1.0536,  ..., -1.3096, -1.2811, -1.1958],
          [-1.0678, -0.9541, -1.0394,  ..., -1.2954, -1.2527, -1.0394],
          [-1.0252, -0.9541, -0.9967,  ..., -1.2811, -1.2100, -1.1532]]]])}
</pre></div>
</div>
</div>
</div>
<p>辞書型の <code class="docutils literal notranslate"><span class="pre">output</span></code> に含まれている <code class="docutils literal notranslate"><span class="pre">pixel_values</span></code> が前処理済みの画像です。以下のようにしてデータのサイズを確認すると、CLIP の入力に適した <code class="docutils literal notranslate"><span class="pre">224</span> <span class="pre">x</span> <span class="pre">224</span></code> のサイズになっていることがわかります。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span><span class="p">[</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 3, 224, 224])
</pre></div>
</div>
</div>
</div>
</section>
<section id="cliptokenizer">
<h3>CLIPTokenizer による言語処理<a class="headerlink" href="#cliptokenizer" title="Permalink to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">CLIPProcessor</span></code> に含まれる言語処理機構 <code class="docutils literal notranslate"><span class="pre">CLIPTokenizer</span></code> の動作を確認します。ここでは <code class="docutils literal notranslate"><span class="pre">CLIPProcessor</span></code> の <code class="docutils literal notranslate"><span class="pre">text</span></code> 引数の対象のテキストを渡すことで、内部的に <code class="docutils literal notranslate"><span class="pre">CLIPTokenizer</span></code> を用いてテキストをトークンに分割します。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s2">&quot;Hello world&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">output</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;input_ids&#39;: tensor([[49406,  3306,  1002, 49407]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1]])}
</pre></div>
</div>
</div>
</div>
<p>辞書型の <code class="docutils literal notranslate"><span class="pre">output</span></code> には、分割結果はそれぞれトークンの ID 列に変換され、<code class="docutils literal notranslate"><span class="pre">input_ids</span></code> というキーで格納されます。<code class="docutils literal notranslate"><span class="pre">attention_mask</span></code> というキーは、可変長のテキストに対応するための mask を表しています。長さが異なるテキストが processor に入力されたときに、padding 部分に mask するように 0/1 の値が返ってきます。</p>
<p>トークンの ID 列である <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> は、processor のデコード機能を使って文字列に戻すことができます。<code class="docutils literal notranslate"><span class="pre">output</span></code> はバッチを想定したデータ構造（<code class="docutils literal notranslate"><span class="pre">input_ids</span></code> のサイズが <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>）になっているため、以下では <a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPProcessor.batch_decode" rel="noreferrer" target="_blank"><code class="docutils literal notranslate"><span class="pre">processor.batch_decode</span></code></a> を使用してデコードしています。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">processor</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;&lt;|startoftext|&gt;hello world &lt;|endoftext|&gt;&#39;]
</pre></div>
</div>
</div>
</div>
<p>CLIP では大文字・小文字を区別しない tokenizer を使用しているため、大文字だったところが小文字になっている点を除いては、もともとのテキストを復元できているように見えます。</p>
<p>CLIP で使用している tokenizer を始め、多くの tokenizer は学習用にテキストやセンテンスの始めを示す <code class="docutils literal notranslate"><span class="pre">&lt;|startoftext|&gt;</span></code> や <code class="docutils literal notranslate"><span class="pre">&lt;BOS&gt;</span></code> (begin of sentence) といったトークンや、テキストの終わりを示す <code class="docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code> や <code class="docutils literal notranslate"><span class="pre">&lt;EOS&gt;</span></code> (end of sentence) といったトークンを自動的に挿入します。</p>
</section>
</section>
<section id="id2">
<h2>画像とテキストのセットアップ<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h2>
<p>以下、8枚の画像とそのテキストキャプションを CLIP モデルに与えて、対応する特徴ベクトルの類似度を比較していきます。</p>
<p>再度確認ですが、CLIP の tokenizer は大文字・小文字を区別しないので、以下のようにざっくりとした文章を自由に記述可能です。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#</span>
<span class="c1"># 使用する skimage の画像とその説明文</span>
<span class="c1">#</span>
<span class="n">descriptions_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;page&quot;</span><span class="p">:</span> <span class="s2">&quot;a page of text about segmentation&quot;</span><span class="p">,</span>
    <span class="s2">&quot;chelsea&quot;</span><span class="p">:</span> <span class="s2">&quot;a facial photo of a tabby cat&quot;</span><span class="p">,</span>
    <span class="s2">&quot;astronaut&quot;</span><span class="p">:</span> <span class="s2">&quot;a portrait of an astronaut with the American flag&quot;</span><span class="p">,</span>
    <span class="s2">&quot;rocket&quot;</span><span class="p">:</span> <span class="s2">&quot;a rocket standing on a launchpad&quot;</span><span class="p">,</span>
    <span class="s2">&quot;motorcycle_right&quot;</span><span class="p">:</span> <span class="s2">&quot;a red motorcycle standing in a garage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;camera&quot;</span><span class="p">:</span> <span class="s2">&quot;a person looking at a camera on a tripod&quot;</span><span class="p">,</span>
    <span class="s2">&quot;horse&quot;</span><span class="p">:</span> <span class="s2">&quot;a black-and-white silhouette of a horse&quot;</span><span class="p">,</span>
    <span class="s2">&quot;coffee&quot;</span><span class="p">:</span> <span class="s2">&quot;a cup of coffee on a saucer&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://scikit-image.org/" rel="noreferrer" target="_blank"><code class="docutils literal notranslate"><span class="pre">skimage</span></code></a> から画像データを取得します。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">skimage</span>

<span class="kn">from</span> <span class="nn">more_itertools</span> <span class="kn">import</span> <span class="n">sort_together</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">original_imgs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">original_txts</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># skimage から .png か .jpg な画像のパスを習得する</span>
<span class="n">filenames</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">filename</span> <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">skimage</span><span class="o">.</span><span class="n">data_dir</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">filename</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.png&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">filename</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.jpg&quot;</span><span class="p">)</span>
<span class="p">]</span>
<span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">filenames</span><span class="p">:</span>
    <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">descriptions_dict</span><span class="p">:</span>
        <span class="k">continue</span>

    <span class="c1"># 画像の読み込み</span>
    <span class="n">image_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">skimage</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&quot;RGB&quot;</span><span class="p">)</span>
    <span class="n">original_imgs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

    <span class="n">text</span> <span class="o">=</span> <span class="n">descriptions_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
    <span class="n">original_txts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># 画像とテキストの数があっているか確認</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">original_txts</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">original_imgs</span><span class="p">)</span>
<span class="c1"># テキストの文字列をベースに、テキストと画像のリストをソートする</span>
<span class="n">original_txts</span><span class="p">,</span> <span class="n">original_imgs</span> <span class="o">=</span> <span class="n">sort_together</span><span class="p">((</span><span class="n">original_txts</span><span class="p">,</span> <span class="n">original_imgs</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>読み込んだ画像とテキストのペアを確認してみます。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">nrows</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span> <span class="n">ncols</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">ncols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nrows</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ncols</span><span class="p">):</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">original_imgs</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">ncols</span> <span class="o">+</span> <span class="n">j</span><span class="p">])</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">original_txts</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">ncols</span> <span class="o">+</span> <span class="n">j</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/3a2717036f582dbdafbda1a1b971b5bcb4092787230a88ec3d7ca0abd7a55a74.png" src="../_images/3a2717036f582dbdafbda1a1b971b5bcb4092787230a88ec3d7ca0abd7a55a74.png" />
</div>
</div>
</section>
<section id="id3">
<h2>特徴量の構築<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h2>
<p>CLIP モデルに画像とテキストを入力するため、<code class="docutils literal notranslate"><span class="pre">CLIPProcessor</span></code> を使用します。この processor は煩雑な複数のモダリティの前処理を以下の一行で完了します。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">original_txts</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="n">original_imgs</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">inputs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;input_ids&#39;: tensor([[49406,   320,  1449,   268,   537,   268,  1579, 26149,   539,   320,
          4558, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407],
        [49406,   320,  1937,   539,  2453,   525,   320, 42272, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407],
        [49406,   320, 11909,  1125,   539,   320, 36145,  2368, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407],
        [49406,   320,  2504,   539,  4160,   781, 10551,  9512, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407],
        [49406,   320,  2533,  1312,   536,   320,  3934,   525,   320, 36141,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407],
        [49406,   320,  5352,   539,   550, 18376,   593,   518,  2151,  4859,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407],
        [49406,   320,   736, 10297,  2862,   530,   320,  8474, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407],
        [49406,   320,  8383,  2862,   525,   320, 31168,  7601, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0]]), &#39;pixel_values&#39;: tensor([[[[ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],
          [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],
          [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],
          ...,
          [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],
          [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],
          [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303]],

         [[ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],
          [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],
          [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],
          ...,
          [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],
          [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],
          [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749]],

         [[ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],
          [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],
          [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],
          ...,
          [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],
          [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],
          [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459]]],


        [[[-1.2229, -1.1937, -1.2083,  ...,  1.1858,  1.2150,  1.2588],
          [-1.2083, -1.1937, -1.2083,  ...,  1.2442,  1.2296,  1.2150],
          [-1.2083, -1.1937, -1.1937,  ...,  1.2150,  1.0836,  0.9668],
          ...,
          [ 1.7114,  1.7260,  1.7406,  ...,  0.6895,  1.0398,  0.9084],
          [ 1.7114,  1.7114,  1.7406,  ...,  0.9960,  0.8647,  0.6895],
          [ 1.7114,  1.7552,  1.7114,  ...,  0.7917,  0.6311,  1.0398]],

         [[-1.3619, -1.3319, -1.3619,  ...,  0.1689,  0.1989,  0.2740],
          [-1.3469, -1.3319, -1.3619,  ...,  0.1989,  0.2289,  0.2289],
          [-1.3469, -1.3319, -1.3319,  ...,  0.2139,  0.1239,  0.0038],
          ...,
          [ 1.2945,  1.3695,  1.4145,  ..., -0.4764, -0.1463, -0.2063],
          [ 1.3395,  1.3995,  1.4145,  ..., -0.2063, -0.2063, -0.4614],
          [ 1.3095,  1.3845,  1.3395,  ..., -0.2663, -0.4614, -0.1913]],

         [[-1.2527, -1.2385, -1.2527,  ..., -0.4279, -0.3853, -0.3426],
          [-1.2527, -1.2385, -1.2527,  ..., -0.3853, -0.3568, -0.3284],
          [-1.2669, -1.2385, -1.2527,  ..., -0.3568, -0.3995, -0.4848],
          ...,
          [ 0.8519,  0.9656,  1.0225,  ..., -0.9256, -0.7266, -0.7692],
          [ 0.9372,  1.0225,  1.0083,  ..., -0.7408, -0.7692, -0.9256],
          [ 0.9230,  1.0225,  0.9941,  ..., -0.7977, -0.8545, -0.7692]]],


        [[[-0.0259, -0.0113,  0.0471,  ...,  0.5289,  0.5873,  0.6457],
          [ 0.0471, -0.0259,  0.0909,  ...,  0.6165,  0.6311,  0.5727],
          [ 0.0617, -0.0259,  0.0909,  ...,  0.6311,  0.5581,  0.5143],
          ...,
          [ 0.9376,  0.9230,  0.9376,  ...,  0.8355,  0.8063,  0.7479],
          [ 0.9376,  0.9084,  0.9230,  ...,  0.8355,  0.7771,  0.7333],
          [ 0.9376,  0.9084,  0.9376,  ...,  0.8209,  0.7771,  0.7333]],

         [[-0.8066, -0.7316, -0.7016,  ...,  0.0188,  0.0789,  0.1689],
          [-0.7166, -0.7766, -0.6715,  ...,  0.0939,  0.1389,  0.0789],
          [-0.6865, -0.7766, -0.6415,  ...,  0.1539,  0.0488, -0.0112],
          ...,
          [ 0.5741,  0.5591,  0.5741,  ...,  0.6191,  0.5891,  0.5291],
          [ 0.5741,  0.5441,  0.5591,  ...,  0.6191,  0.5591,  0.5291],
          [ 0.5441,  0.5441,  0.5741,  ...,  0.6041,  0.5591,  0.4991]],

         [[-0.7834, -0.7123, -0.6412,  ...,  0.1266,  0.1977,  0.2688],
          [-0.6981, -0.7408, -0.6128,  ...,  0.2120,  0.2546,  0.1835],
          [-0.6555, -0.7266, -0.5844,  ...,  0.2546,  0.1693,  0.1124],
          ...,
          [ 0.5248,  0.4964,  0.5248,  ...,  0.5817,  0.5532,  0.5106],
          [ 0.5248,  0.4821,  0.5106,  ...,  0.5675,  0.5390,  0.5248],
          [ 0.5248,  0.4964,  0.5532,  ...,  0.5817,  0.5532,  0.5248]]],


        ...,


        [[[ 0.2953, -0.3616,  0.1931,  ...,  0.0471,  0.0471,  0.0179],
          [ 1.2588,  1.0252,  1.1420,  ...,  0.0617,  0.0325,  0.0471],
          [ 1.6530,  1.5362,  1.3172,  ...,  0.0471,  0.0471,  0.0325],
          ...,
          [ 0.9230,  0.9522,  0.8501,  ..., -0.6828, -0.9018, -1.7923],
          [ 0.9230,  0.8792,  0.6749,  ..., -0.6390, -1.6755, -1.7923],
          [ 0.8938,  0.7771,  0.4997,  ..., -0.8434, -1.2667, -1.7923]],

         [[ 0.3190, -0.2963,  0.2589,  ...,  0.0338,  0.0338,  0.0038],
          [ 1.2945,  1.0694,  1.2044,  ...,  0.0038,  0.0038,  0.0038],
          [ 1.6697,  1.5796,  1.3395,  ...,  0.0188,  0.0188,  0.0038],
          ...,
          [ 0.7692,  0.8292,  0.6792,  ..., -0.7466, -0.8816, -1.7521],
          [ 0.7842,  0.7392,  0.5141,  ..., -0.6715, -1.6470, -1.7521],
          [ 0.7692,  0.6191,  0.3190,  ..., -0.8516, -1.2118, -1.7521]],

         [[ 0.5959,  0.1977,  0.4253,  ...,  0.1124,  0.1124,  0.0698],
          [ 1.3638,  1.1789,  1.2216,  ...,  0.0982,  0.0982,  0.0982],
          [ 1.7193,  1.6340,  1.3780,  ...,  0.1124,  0.1124,  0.0982],
          ...,
          [ 1.0083,  1.0510,  0.8945,  ..., -0.5559, -0.6697, -1.4802],
          [ 1.0083,  0.9514,  0.6812,  ..., -0.5275, -1.4091, -1.4802],
          [ 0.9656,  0.8234,  0.4537,  ..., -0.6555, -1.0394, -1.4802]]],


        [[[-0.5806, -0.4054, -0.3470,  ..., -0.0113, -0.0259, -0.0405],
          [-0.5806, -0.8142, -0.8872,  ..., -0.0550, -0.0405, -0.0405],
          [-0.5952, -0.7412, -0.8288,  ..., -0.0842, -0.0696, -0.0696],
          ...,
          [ 0.6749,  0.7771,  0.8355,  ...,  0.6165,  0.6165,  0.5873],
          [ 0.7041,  0.7333,  0.7479,  ...,  0.6165,  0.5873,  0.5581],
          [ 0.6749,  0.6749,  0.7041,  ...,  0.6311,  0.6019,  0.5727]],

         [[-1.0617, -0.8516, -0.6715,  ...,  0.0188,  0.0038,  0.0188],
          [-1.0317, -1.3169, -1.4519,  ...,  0.0188,  0.0038,  0.0188],
          [-1.0617, -1.3169, -1.4369,  ..., -0.0112,  0.0038,  0.0038],
          ...,
          [ 0.6341,  0.7392,  0.7692,  ...,  0.4540,  0.4390,  0.4090],
          [ 0.6191,  0.6642,  0.6942,  ...,  0.4390,  0.4390,  0.3790],
          [ 0.6191,  0.6191,  0.6491,  ...,  0.4540,  0.4390,  0.4090]],

         [[-1.0821, -0.8830, -0.5844,  ...,  0.2831,  0.2973,  0.2973],
          [-1.0821, -1.2669, -1.3807,  ...,  0.3257,  0.3399,  0.3257],
          [-1.0963, -1.2811, -1.3807,  ...,  0.2831,  0.2831,  0.2973],
          ...,
          [ 0.6244,  0.7381,  0.7808,  ...,  0.4679,  0.4253,  0.4253],
          [ 0.6386,  0.6670,  0.6955,  ...,  0.4537,  0.4395,  0.3826],
          [ 0.6386,  0.6386,  0.6528,  ...,  0.4679,  0.4537,  0.4110]]],


        [[[-1.5003, -1.5003, -1.5003,  ..., -1.5733, -1.6025, -1.6025],
          [-1.5003, -1.5003, -1.5003,  ..., -1.5733, -1.6025, -1.6025],
          [-1.5003, -1.5003, -1.5003,  ..., -1.5733, -1.6025, -1.5879],
          ...,
          [ 1.1274,  1.7698,  1.6092,  ..., -1.3689, -1.3689, -1.3835],
          [ 1.1858,  1.2442,  0.4559,  ..., -1.3835, -1.3835, -1.3981],
          [ 0.9084,  0.3975,  0.2807,  ..., -1.4273, -1.4273, -1.4127]],

         [[-1.2118, -1.2118, -1.2118,  ..., -1.3619, -1.3469, -1.3619],
          [-1.2118, -1.2118, -1.2118,  ..., -1.3469, -1.3469, -1.3619],
          [-1.2118, -1.2118, -1.2118,  ..., -1.3619, -1.3469, -1.3469],
          ...,
          [ 1.1894,  1.8948,  1.6847,  ..., -1.2869, -1.2869, -1.3019],
          [ 1.2945,  1.3995,  0.5141,  ..., -1.3019, -1.3019, -1.3169],
          [ 0.9343,  0.4540,  0.3490,  ..., -1.3169, -1.3169, -1.3319]],

         [[-0.5986, -0.5986, -0.5986,  ..., -0.8545, -0.8403, -0.8403],
          [-0.5986, -0.5986, -0.5986,  ..., -0.8403, -0.8261, -0.8261],
          [-0.5986, -0.5986, -0.5986,  ..., -0.8261, -0.8261, -0.8261],
          ...,
          [ 0.9941,  1.8046,  1.6198,  ..., -0.8688, -0.8688, -0.8830],
          [ 0.8519,  0.9799,  0.2831,  ..., -0.8830, -0.8830, -0.8972],
          [ 0.4537,  0.0698, -0.0440,  ..., -0.9114, -0.9114, -0.9114]]]])}
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">CLIPModel</span></code> を利用して、画像特徴とテキスト特徴をそれぞれ計算します。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">img_features</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_image_features</span><span class="p">(</span>
        <span class="n">pixel_values</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="n">txt_features</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_text_features</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">],</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id4">
<h2>コサイン類似度を計算する<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h2>
<p>上記で得られた特徴量を正規化し、各ペアの内積を計算します。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">img_features</span> <span class="o">=</span> <span class="n">img_features</span> <span class="o">/</span> <span class="n">img_features</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">txt_features</span> <span class="o">=</span> <span class="n">txt_features</span> <span class="o">/</span> <span class="n">txt_features</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">similarity</span> <span class="o">=</span> <span class="n">img_features</span> <span class="o">@</span> <span class="n">txt_features</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
</div>
<p>画像とテキスト、それぞれの類似度を以下のようにして可視化します。対角線上がもともと対応していた画像とテキストにおける類似度です。CLIP による特徴ベクトルを利用することで、画像とテキストの正しいペアの類似度が高くなっていることが確認できました。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">original_imgs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">original_txts</span><span class="p">)</span>
<span class="n">count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">original_imgs</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">14</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">similarity</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">original_txts</span><span class="p">)),</span> <span class="n">labels</span><span class="o">=</span><span class="n">original_txts</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">original_imgs</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6</span><span class="p">),</span> <span class="n">origin</span><span class="o">=</span><span class="s2">&quot;lower&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">similarity</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">similarity</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">similarity</span><span class="p">[</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="k">for</span> <span class="n">side</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="s2">&quot;top&quot;</span><span class="p">,</span> <span class="s2">&quot;right&quot;</span><span class="p">,</span> <span class="s2">&quot;bottom&quot;</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="n">side</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">count</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="n">count</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">])</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Cosine similarity between text and image features&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Cosine similarity between text and image features&#39;)
</pre></div>
</div>
<img alt="../_images/8dfb28bbe3a8c6dfd2dae804e8155e2d413ea025a955ffa483743b889700b494.png" src="../_images/8dfb28bbe3a8c6dfd2dae804e8155e2d413ea025a955ffa483743b889700b494.png" />
</div>
</div>
</section>
<section id="zero-shot">
<h2>Zero-shot 画像分類<a class="headerlink" href="#zero-shot" title="Permalink to this heading">#</a></h2>
<p>CLIP は追加学習なしで未知の画像を分類することが可能です。以下では CIFAR100 と呼ばれる 100 クラスの画像分類データセットを用いて CLIP の zero-shot 画像分類性能について見ていきます。</p>
<p>まずは <a class="reference external" href="https://pytorch.org/vision/stable/index.html" rel="noreferrer" target="_blank"><code class="docutils literal notranslate"><span class="pre">torchvision</span></code></a> を用いて CIFAR100 データセットを読み込みます。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR100</span>

<span class="n">cifar100</span> <span class="o">=</span> <span class="n">CIFAR100</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s2">&quot;~/.cache&quot;</span><span class="p">),</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /root/.cache/cifar-100-python.tar.gz
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 169001437/169001437 [00:13&lt;00:00, 12737679.44it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting /root/.cache/cifar-100-python.tar.gz to /root/.cache
</pre></div>
</div>
</div>
</div>
<p>CLIP に入力するプロンプトとして、<code class="docutils literal notranslate"><span class="pre">This</span> <span class="pre">is</span> <span class="pre">a</span> <span class="pre">photo</span> <span class="pre">of</span> <span class="pre">a</span> <span class="pre">{label}</span></code> のようなテンプレート文を用意します。その後 CIFAR100 に存在するクラスでテンプレートを埋めた文のリストを得ます。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">text_template</span> <span class="o">=</span> <span class="s2">&quot;This is a photo of a </span><span class="si">{label}</span><span class="s2">&quot;</span>
<span class="n">text_descriptions</span> <span class="o">=</span> <span class="p">[</span><span class="n">text_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">cifar100</span><span class="o">.</span><span class="n">classes</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>上記で用意したテキストのリストを元に、<code class="docutils literal notranslate"><span class="pre">CLIPProcessor</span></code> である processor でトークンへ変換します。その後 <code class="docutils literal notranslate"><span class="pre">CLIPModel</span></code> でテキストトークン列からテキスト特徴ベクトルを取得します。最後に取得したテキストベクトルを正規化しておきます。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text_descriptions</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">txt_features</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_text_features</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="n">txt_features</span> <span class="o">=</span> <span class="n">txt_features</span> <span class="o">/</span> <span class="n">txt_features</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>画像とテキストの特徴ベクトルの内積を取り、そのスコアを 100 倍して softmax に通すことで、通常の分類モデルのような形で画像を分類できます。ここでは予測結果上位 5 件を選択しています。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">txt_probs</span> <span class="o">=</span> <span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">img_features</span> <span class="o">@</span> <span class="n">txt_features</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">top_probs</span><span class="p">,</span> <span class="n">top_labels</span> <span class="o">=</span> <span class="n">txt_probs</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>これまでに得られた zero-shot 画像分類結果を以下に可視化します。画像とそれに紐づくキャプションと、zero-shot 分類したときの予測結果上位 5 件を示しています。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nrows</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span> <span class="n">ncols</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;ggplot&quot;</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">top_probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">original_imgs</span><span class="p">):</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">original_txts</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">top_probs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">invert_yaxis</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_axisbelow</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="n">cifar100</span><span class="o">.</span><span class="n">classes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">top_labels</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Probability&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/970d2b61347d4d7a6a147d83c34c38687ff3bd8e8335112ee6c871474ccdcefd.png" src="../_images/970d2b61347d4d7a6a147d83c34c38687ff3bd8e8335112ee6c871474ccdcefd.png" />
</div>
</div>
<p>男 (man)・女 (woman) のレベルでは zero-shot で適切に予測が可能なように見えます。猫が虎として予測されてしまっているのは仕方がなさそうですね。カップやバイク、ロケットが高い確信度で予測が出力されているのは素晴らしいですね。</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="section-03-11.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">前へ</p>
        <p class="prev-next-title">画像生成 AI 入門: Python による拡散モデルの理論と実践</p>
      </div>
    </a>
    <a class="right-next"
       href="section-06-18.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">次へ</p>
        <p class="prev-next-title">画像生成 AI 入門: Python による拡散モデルの理論と実践</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> 目次
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-04-key-researches-based-on-non-diffusion-models">Section 04. Key Researches Based on Non-Diffusion Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lecture-12-about-clip-contrastive-language-image-pre-training">Lecture 12. About CLIP (Contrastive Language-Image Pre-training)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">セットアップ</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu">GPU が使用できるか確認</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python">利用する Python ライブラリをインストール</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clip">CLIP モデルの読み込み</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clipprocessor">CLIPProcessor について</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clipimageprocessor">CLIPImageProcessor による画像処理</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cliptokenizer">CLIPTokenizer による言語処理</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">画像とテキストのセットアップ</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">特徴量の構築</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">コサイン類似度を計算する</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot">Zero-shot 画像分類</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
著者 リサーチサイエンティスト 北田俊輔
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>